pip install pygame



import pygame

# Set up the game window
WINDOW_WIDTH = 600
WINDOW_HEIGHT = 800
pygame.init()
game_window = pygame.display.set_mode((WINDOW_WIDTH, WINDOW_HEIGHT))
pygame.display.set_caption("Pac-Man with AI Ghosts")



class PacMan(pygame.sprite.Sprite):
    def __init__(self):
        super().__init__()
        self.image = pygame.image.load("pacman.png").convert_alpha()
        self.rect = self.image.get_rect()
        self.rect.x = WINDOW_WIDTH / 2
        self.rect.y = WINDOW_HEIGHT / 2
        self.speed = 5

    def update(self):
        keys = pygame.key.get_pressed()
        if keys[pygame.K_LEFT]:
            self.rect.x -= self.speed
        elif keys[pygame.K_RIGHT]:
            self.rect.x += self.speed
        elif keys[pygame.K_UP]:
            self.rect.y -= self.speed
        elif keys[pygame.K_DOWN]:
            self.rect.y += self.speed



class Ghost(pygame.sprite.Sprite):
    def __init__(self, color):
        super().__init__()
        self.color = color
        self.image = pygame.Surface([20, 20])
        self.image.fill(color)
        self.rect = self.image.get_rect()
        self.rect.x = WINDOW_WIDTH / 2 - 10
        self.rect.y = WINDOW_HEIGHT / 2 - 10
        self.speed = 3

    def update(self, target):
        dx = target.rect.x - self.rect.x
        dy = target.rect.y - self.rect.y
        distance = math.hypot(dx, dy)
        dx, dy = dx / distance, dy / distance
        self.rect.x += dx * self.speed
        self.rect.y += dy * self.speed



class QGhost(Ghost):
    def __init__(self, color, learning_rate=0.1, discount_factor=0.9, exploration_rate=0.1):
        super().__init__(color)
        self.q_table = defaultdict(lambda: np.zeros(4))
        self.learning_rate = learning_rate
        self.discount_factor = discount_factor
        self.exploration_rate = exploration_rate

    def update(self, target):
        state = self.get_state(target)
        action = self.get_action(state)
        reward = self.get_reward(target)

        next_state = self.get_state(target)
        next_action = self.get_action(next_state)

        q_value = self.q_table[state][action]
        next_q_value = self.q_table[next_state][next_action]
        td_error = reward + self.discount_factor * next_q_value - q_value
        self.q_table[state][action] += self.learning_rate * td_error

        dx, dy = self.get_direction(action)
        self.rect.x += dx * self.speed
        self.rect.y += dy * self.speed

    def get_state(self, target):
        dx = target.rect.x - self.rect.x
        dy = target.rect.y - self.rect.y
        state = round(dx / 10), round(dy / 10)
        return state

    def get_action(self, state):
        if np.random.rand() < self.exploration_rate:
            action = np.random.randint(0, 4)
        else:
            action = np.argmax(self.q_table[state])
        return action

    def get_reward(self, target):
        dx = target.rect.x - self.rect.x
        dy = target.rect.y - self.rect.y
        distance = math.hypot(dx, dy)
        reward = -distance
        return reward

    def get_direction(self, action):
        if action == 0:
            return 1, 0
        elif action == 1:
            return -1, 0
        elif action == 2:
            return 0, 1
        else:
            return 0, -1



from keras.models import Sequential
from keras.layers import Dense, Activation
from keras.optimizers import Adam

class NNGhost(Ghost):
    def __init__(self, color):
        super().__init__(color)
        self.model = self.create_model()

    def create_model(self):
        model = Sequential()
        model.add(Dense(32, input_shape=(2,), activation='relu'))
        model.add(Dense(32, activation='relu'))
        model.add(Dense(4, activation='linear'))
        model.compile(loss='mse', optimizer=Adam(lr=0.001))
        return model

    def update(self, target):
        state = self.get_state(target)
        action = self.get_action(state)
        reward = self.get_reward(target)

        next_state = self.get_state(target)
        next_action = self.get_action(next_state)

        q_values = self.model.predict(np.array([state]))
        q_values[0][action] = reward + 0.9 * np.max(self.model.predict(np.array([next_state])))

        self.model.fit(np.array([state]), q_values, verbose=0)

        dx, dy = self.get_direction(action)
        self.rect.x += dx * self.speed
        self.rect.y += dy * self.speed

    def get_state(self, target):
        dx = target.rect.x - self.rect.x
        dy = target.rect.y - self.rect.y
        state = np.array([dx, dy]) / 10
        return state

    def get_action(self, state):
        q_values = self.model.predict(np.array([state]))
        return np.argmax(q_values[0])

    def get_reward(self, target):
        dx = target.rect.x - self.rect.x
        dy = target.rect.y - self.rect.y
        distance = math.hypot(dx, dy)
        reward = -distance
        return reward

    def get_direction(self, action):
        if action == 0:
            return 1, 0
        elif action == 1:
            return -1, 0
        elif action == 2:
            return 0, 1
        else:
            return 0, -1



    def move_ghost(self, action):
        dx, dy = 0, 0
        if action == 'up': dy = -1
        elif action == 'down': dy = 1
        elif action == 'left': dx = -1
        elif action == 'right': dx = 1

        new_x = max(0, min(self.grid_size - 1, self.ghost_pos[0] + dx))
        new_y = max(0, min(self.grid_size - 1, self.ghost_pos[1] + dy))

        if (new_x, new_y) not in self.walls:
            self.ghost_pos = [new_x, new_y]

        self.steps += 1

        if self.ghost_pos == self.pacman_pos:
            self.caught = True



import random
random.seed(42)

class PacManGame:
    def __init__(self, grid_size=10):
        self.grid_size = grid_size
        self.generate_walls()
        self.reset()

    def reset(self):
        # All possible positions
        positions = [(x, y) for x in range(self.grid_size) for y in range(self.grid_size)]
        # Ghost 1 start
        self.ghost1_pos = list(random.choice(positions))
        # Ghost 2 start (different)
        self.ghost2_pos = list(random.choice(positions))
        while tuple(self.ghost2_pos) == tuple(self.ghost1_pos):
            self.ghost2_pos = list(random.choice(positions))
        # Pac-Man start (not on either ghost)
        pac = random.choice(positions)
        while pac == tuple(self.ghost1_pos) or pac == tuple(self.ghost2_pos):
            pac = random.choice(positions)
        self.pacman_pos = list(pac)

        self.steps = 0
        self.max_steps = 50
        self.caught = False

    def generate_walls(self):
        self.walls = {
            (3,3),(3,4),(3,5),
            (6,5),(7,5),(8,5),
            (5,1),(5,2),(5,3),
        }

    def over(self):
        return self.steps >= self.max_steps or self.caught

    def ghost_caught_pacman(self):
        return self.caught

    # now takes ghost_id (1 or 2)
    def move_ghost(self, ghost_id, action):
        pos = self.ghost1_pos if ghost_id==1 else self.ghost2_pos

        dx = dy = 0
        if action=='up':    dy = -1
        elif action=='down':dy = 1
        elif action=='left':dx = -1
        elif action=='right':dx = 1

        new_x = max(0, min(self.grid_size-1, pos[0]+dx))
        new_y = max(0, min(self.grid_size-1, pos[1]+dy))

        if (new_x,new_y) not in self.walls:
            pos[0], pos[1] = new_x, new_y

        self.steps += 1
        # if either ghost lands on Pac-Man, caught = True
        if pos == self.pacman_pos:
            self.caught = True

    def move_pacman(self):
        # Smarter policy: move away from the closest ghost
        # compute vectors to each ghost
        dx1 = self.ghost1_pos[0] - self.pacman_pos[0]
        dy1 = self.ghost1_pos[1] - self.pacman_pos[1]
        dx2 = self.ghost2_pos[0] - self.pacman_pos[0]
        dy2 = self.ghost2_pos[1] - self.pacman_pos[1]

        # pick the ghost that's nearer
        dist1 = abs(dx1) + abs(dy1)
        dist2 = abs(dx2) + abs(dy2)
        dx, dy = (dx1, dy1) if dist1 < dist2 else (dx2, dy2)

        # move in opposite direction (away)
        move_choices = []
        if dx > 0: move_choices.append('left')
        if dx < 0: move_choices.append('right')
        if dy > 0: move_choices.append('up')
        if dy < 0: move_choices.append('down')
        if not move_choices:
            move_choices = ['stay']

        direction = random.choice(move_choices)
        ddx = ddy = 0
        if direction=='up':    ddy = -1
        elif direction=='down':ddy = 1
        elif direction=='left':ddx = -1
        elif direction=='right':ddx = 1

        new_x = max(0, min(self.grid_size-1, self.pacman_pos[0]+ddx))
        new_y = max(0, min(self.grid_size-1, self.pacman_pos[1]+ddy))
        if (new_x,new_y) not in self.walls:
            self.pacman_pos = [new_x,new_y]



#import random
#random.seed(42)


#class PacManGame:
 #   def __init__(self, grid_size=10):
  #      self.grid_size = grid_size
   #     self.reset()
    #    self.generate_walls()  
    #def reset(self):
        # Generate all possible positions as (x,y) tuples
     #   positions = [(x, y) for x in range(self.grid_size) for y in range(self.grid_size)]
        # Pick random start but convert to list so it's mutable
      #  self.ghost_pos = list(random.choice(positions))
       # pac = random.choice(positions)
        # Make sure Pac-Man doesn't start on the ghost
        #while pac == tuple(self.ghost_pos):
         #   pac = random.choice(positions)
        #self.pacman_pos = list(pac)

        #self.steps = 0
        #self.max_steps = 50
        #self.caught = False

    #def generate_walls(self):
        # Example fixed walls (customize this)
       # self.walls = set([
        #    (3, 3), (3, 4), (3, 5),
         #   (6, 5), (7, 5), (8, 5),
          #  (5, 1), (5, 2), (5, 3),
        #])

    #def over(self):
     #   return self.steps >= self.max_steps or self.caught

    #def ghost_caught_pacman(self):
     #   return self.caught

    #def move_ghost(self, action):
        
     #   dx = dy = 0
      #  if action == 'up':    dy = -1
        #elif action == 'down':dy = 1
       # elif action == 'left':dx = -1
        #elif action == 'right':dx = 1

    # Calculate proposed new position
        #new_x = max(0, min(self.grid_size - 1, self.ghost_pos[0] + dx))
        #new_y = max(0, min(self.grid_size - 1, self.ghost_pos[1] + dy))

    # Only move if no wall
        #if (new_x, new_y) not in self.walls:
         #   self.ghost_pos = [new_x, new_y]

        #self.steps += 1
  
        #if self.ghost_pos == self.pacman_pos:
         #   self.caught = True

    #def move_pacman(self):
        #direction = random.choice(['up','down','left','right','stay'])
     #   dx = dy = 0
      #  if direction == 'up':    dy = -1
       # elif direction == 'down':dy = 1
        #elif direction == 'left':dx = -1
        #elif direction == 'right':dx = 1
        # Update Pac-Man position
        #self.pacman_pos[0] = max(0, min(self.grid_size-1, self.pacman_pos[0] + dx))
        #self.pacman_pos[1] = max(0, min(self.grid_size-1, self.pacman_pos[1] + dy))



class QGhost:
    def __init__(self, actions=['up', 'down', 'left', 'right'], alpha=0.1, gamma=0.9, epsilon=0.2):
        self.q_table = {}
        self.alpha = alpha
        self.gamma = gamma
        self.epsilon = epsilon
        self.actions = actions

    def get_state(self, ghost_pos, pacman_pos):
        dx = pacman_pos[0] - ghost_pos[0]
        dy = pacman_pos[1] - ghost_pos[1]
        return (dx, dy)
    def choose_action(self, state):
        if state not in self.q_table:
            self.q_table[state] = {a: 0 for a in self.actions}

        if random.random() < self.epsilon:
            return random.choice(self.actions)
        else:
            return max(self.q_table[state], key=self.q_table[state].get)

    def learn(self, state, action, reward, next_state):
        if next_state not in self.q_table:
            self.q_table[next_state] = {a: 0 for a in self.actions}
        predict = self.q_table[state][action]
        target = reward + self.gamma * max(self.q_table[next_state].values())
        self.q_table[state][action] += self.alpha * (target - predict)




# training 
game = PacManGame()
ghost1 = QGhost()
ghost2 = QGhost()

episodes = 1000
episode_steps = []
episode_rewards = []
win_rate_history = []
batch_size = 100
wins_in_batch = 0

for episode in range(1, episodes + 1):  # start from 1 for cleaner display
    game.reset()
    total_reward = 0
    steps = 0  # âœ… count how many steps this episode takes

    while not game.over():
        game.move_pacman()

        # --- Ghost 1 ---
        state1  = ghost1.get_state(game.ghost1_pos, game.pacman_pos)
        action1 = ghost1.choose_action(state1)
        game.move_ghost(1, action1)
        next_state1 = ghost1.get_state(game.ghost1_pos, game.pacman_pos)
        reward1 = 10 if game.ghost_caught_pacman() else -1
        total_reward += reward1   # accumulate ghost1â€™s reward
        ghost1.learn(state1, action1, reward1, next_state1)

        steps += 1              # âœ… count steps (shared)

        if game.over(): 
            break

        # --- Ghost 2 ---
        state2  = ghost2.get_state(game.ghost2_pos, game.pacman_pos)
        action2 = ghost2.choose_action(state2)
        game.move_ghost(2, action2)
        next_state2 = ghost2.get_state(game.ghost2_pos, game.pacman_pos)
        reward2 = 10 if game.ghost_caught_pacman() else -1
        total_reward += reward2   # accumulate ghost2â€™s reward
        ghost2.learn(state2, action2, reward2, next_state2)

    # âœ… After episode ends, log metrics
    episode_steps.append(steps)
    episode_rewards.append(total_reward)

    if game.ghost_caught_pacman():
        wins_in_batch += 1

    # âœ… After every batch of episodes (e.g. 100), log win rate
    if episode % batch_size == 0:
        win_rate = wins_in_batch / batch_size
        win_rate_history.append(win_rate)
        print(f"Episode {episode} â†’ Win rate in last {batch_size}: {win_rate:.2f}")
        wins_in_batch = 0  # reset for next batch



import matplotlib.pyplot as plt

batches = list(range(batch_size, episodes + 1, batch_size))
plt.plot(batches, win_rate_history, marker='o')
plt.xlabel("Episodes")
plt.ylabel("Win Rate (per batch of 100)")
plt.title("Learning Curve: Ghost Win Rate Over Time")
plt.grid(True)
plt.show()



# Disable exploration for testing
ghost1.epsilon = 0
ghost2.epsilon = 0
successes = 0
test_episodes = 100

for episode in range(test_episodes):
    game.reset()
    while not game.over():
        game.move_pacman()

        # Ghost 1 acts
        state1 = ghost1.get_state(game.ghost1_pos, game.pacman_pos)
        action1 = ghost1.choose_action(state1)
        game.move_ghost(1, action1)

        if game.over():
            break  # stop if caught

        # Ghost 2 acts
        state2 = ghost2.get_state(game.ghost2_pos, game.pacman_pos)
        action2 = ghost2.choose_action(state2)
        game.move_ghost(2, action2)

    if game.ghost_caught_pacman():
        successes += 1

accuracy = (successes / test_episodes) * 100
print(f"Test Accuracy (Pac-Man caught): {accuracy:.2f}%")




import time
from IPython.display import clear_output

def print_grid(game):
    clear_output(wait=True)
    for y in range(game.grid_size):
        row = ''
        for x in range(game.grid_size):
            if (x, y) in game.walls:
                row += '# '
            elif [x, y] == game.ghost1_pos:
                row += '1 '  # Ghost 1
            elif [x, y] == game.ghost2_pos:
                row += '2 '  # Ghost 2
            elif [x, y] == game.pacman_pos:
                row += 'P '  # Pac-Man
            else:
                row += '. '
        print(row)
    print()



# Cell 2: Run and visualize 5 episodes with two ghosts
ghost1.epsilon = 0  # greedy mode
ghost2.epsilon = 0
episodes_to_show = 5

for ep in range(episodes_to_show):
    print(f"\nðŸŽ¬ Episode {ep+1}")
    game = PacManGame()
    game.reset()
    
    while not game.over():
        game.move_pacman()

        # Ghost 1
        state1  = ghost1.get_state(game.ghost1_pos, game.pacman_pos)
        action1 = ghost1.choose_action(state1)
        game.move_ghost(1, action1)

        if game.over():
            break  # don't let ghost2 act if already caught

        # Ghost 2
        state2  = ghost2.get_state(game.ghost2_pos, game.pacman_pos)
        action2 = ghost2.choose_action(state2)
        game.move_ghost(2, action2)

        print_grid(game)
        time.sleep(0.2)

    if game.ghost_caught_pacman():
        print(f"âœ… Pac-Man caught in {game.steps} steps!")
    else:
        print(f"âŒ Pac-Man escaped after {game.steps} steps.")
    time.sleep(1)




